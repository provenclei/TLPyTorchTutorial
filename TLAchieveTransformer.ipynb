{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"动手实现transformer.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"3K1KULvTNygY","colab_type":"code","colab":{}},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math, copy, time\n","from torch.autograd import Variable\n","import matplotlib.pyplot as plt\n","import seaborn\n","seaborn.set_context(context=\"talk\")\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fax0yyAAN986","colab_type":"code","colab":{}},"source":["class EncoderDecoder(nn.Module):\n","  \"\"\"\n","  A standard Encoder-Decoder architecture. Base for this and many \n","  other models.\n","  \"\"\"\n","  def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n","    super(EncoderDecoder, self).__init__()\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    self.src_embed = src_embed\n","    self.tgt_embed = tgt_embed\n","    self.generator = generator\n","\n","  def forward(self, src, tgt, src_mask, tgt_mask):\n","    \"Take in and process masked src and target sequences.\"\n","    return self.decode(self.encode(src, src_mask), src_mask,\n","                          tgt, tgt_mask)\n","\n","  def encode(self, src, src_mask):\n","    return self.encoder(self.src_embed(src), src_mask)\n","\n","  def decode(self, memory, src_mask, tgt, tgt_mask):\n","    ecoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DjUCvlaXO7Y1","colab_type":"code","colab":{}},"source":["class Generator(nn.Module):\n","  \"Define standard linear + softmax generation step.\"\n","  def __init__(self, d_model, vocab):\n","    super(Generator, self).__init__()\n","    self.proj = nn.Linear(d_model, vocab)\n","\n","  def forward(self, x):\n","    return F.log_softmax(self.proj(x), dim=-1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oAI5Sze-O_nz","colab_type":"code","colab":{}},"source":["def clones(module, N):\n","  \"Produce N identical layers.\"\n","  return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AdhdSLIlQlLh","colab_type":"code","colab":{}},"source":["class Encoder(nn.Module):\n","  \"Core encoder is a stack of N layers\"\n","  def __init__(self, layer, N):\n","    super(Encoder, self).__init__()\n","    self.layers = clones(layer, N)\n","    self.norm = LayerNorm(layer.size)\n","\n","  def forward(self, x, mask):\n","    \"Pass the input (and mask) through each layer in turn.\"\n","    for layer in self.layers:\n","        x = layer(x, mask)\n","    return self.norm(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ltJ82zMSQq7j","colab_type":"code","colab":{}},"source":["class LayerNorm(nn.Module):\n","  \"Construct a layernorm module (See citation for details).\"\n","  def __init__(self, features, eps=1e-6):\n","    super(LayerNorm, self).__init__()\n","    self.a_2 = nn.Parameter(torch.ones(features))\n","    self.b_2 = nn.Parameter(torch.zeros(features))\n","    self.eps = eps\n","\n","  def forward(self, x):\n","    mean = x.mean(-1, keepdim=True)\n","    std = x.std(-1, keepdim=True)\n","    return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t5DNujpmQ6lv","colab_type":"code","colab":{}},"source":["class SublayerConnection(nn.Module):\n","  \"\"\"\n","  A residual connection followed by a layer norm.\n","  Note for code simplicity the norm is first as opposed to last.\n","  \"\"\"\n","  def __init__(self, size, dropout):\n","    super(SublayerConnection, self).__init__()\n","    self.norm = LayerNorm(size)\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, x, sublayer):\n","    \"Apply residual connection to any sublayer with the same size.\"\n","    return x + self.dropout(sublayer(self.norm(x)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2rMDnQt3RHmY","colab_type":"code","colab":{}},"source":["class EncoderLayer(nn.Module):\n","  \"Encoder is made up of self-attn and feed forward (defined below)\"\n","  def __init__(self, size, self_attn, feed_forward, dropout):\n","    super(EncoderLayer, self).__init__()\n","    self.self_attn = self_attn\n","    self.feed_forward = feed_forward\n","    self.sublayer = clones(SublayerConnection(size, dropout), 2)\n","    self.size = size\n","\n","  def forward(self, x, mask):\n","    \"Follow Figure 1 (left) for connections.\"\n","    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n","    return self.sublayer[1](x, self.feed_forward)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cApD18P-ROSm","colab_type":"code","colab":{}},"source":["class Decoder(nn.Module):\n","  \"Generic N layer decoder with masking.\"\n","  def __init__(self, layer, N):\n","    super(Decoder, self).__init__()\n","    self.layers = clones(layer, N)\n","    self.norm = LayerNorm(layer.size)\n","\n","  def forward(self, x, memory, src_mask, tgt_mask):\n","    for layer in self.layers:\n","      x = layer(x, memory, src_mask, tgt_mask)\n","    return self.norm(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f9eM56PlRT2x","colab_type":"code","colab":{}},"source":["class DecoderLayer(nn.Module):\n","  \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n","  def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n","    super(DecoderLayer, self).__init__()\n","    self.size = size\n","    self.self_attn = self_attn\n","    self.src_attn = src_attn\n","    self.feed_forward = feed_forward\n","    self.sublayer = clones(SublayerConnection(size, dropout), 3)\n","\n","  def forward(self, x, memory, src_mask, tgt_mask):\n","    \"Follow Figure 1 (right) for connections.\"\n","    m = memory\n","    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n","    x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n","    return self.sublayer[2](x, self.feed_forward)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eyIKD_O_RalD","colab_type":"code","colab":{}},"source":["def subsequent_mask(size):\n","  \"Mask out subsequent positions.\"\n","  attn_shape = (1, size, size)\n","  subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n","  return torch.from_numpy(subsequent_mask) == 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s8WaoJVGReHk","colab_type":"code","outputId":"2846dd80-4001-48ba-bfd9-63a364591128","executionInfo":{"status":"ok","timestamp":1571027244512,"user_tz":-480,"elapsed":933,"user":{"displayName":"lei teng","photoUrl":"","userId":"02755924084880047078"}},"colab":{"base_uri":"https://localhost:8080/","height":342}},"source":["plt.figure(figsize=(5,5))\n","plt.imshow(subsequent_mask(20)[0])\n","None"],"execution_count":12,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAVgAAAFFCAYAAACpJPUFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 360x360 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"xUWrA6ILRh4b","colab_type":"code","colab":{}},"source":["def attention(query, key, value, mask=None, dropout=None):\n","  \"Compute 'Scaled Dot Product Attention'\"\n","  d_k = query.size(-1)\n","  scores = torch.matmul(query, key.transpose(-2, -1)) \\\n","           / math.sqrt(d_k)\n","  if mask is not None:\n","    scores = scores.masked_fill(mask == 0, -1e9)\n","  p_attn = F.softmax(scores, dim = -1)\n","  if dropout is not None:\n","    p_attn = dropout(p_attn)\n","  return torch.matmul(p_attn, value), p_attn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PWz7bdjWRpUV","colab_type":"code","colab":{}},"source":["class MultiHeadedAttention(nn.Module):\n","  def __init__(self, h, d_model, dropout=0.1):\n","    \"Take in model size and number of heads.\"\n","    super(MultiHeadedAttention, self).__init__()\n","    assert d_model % h == 0\n","    # We assume d_v always equals d_k\n","    self.d_k = d_model // h\n","    self.h = h\n","    self.linears = clones(nn.Linear(d_model, d_model), 4)\n","    self.attn = None\n","    self.dropout = nn.Dropout(p=dropout)\n","\n","  def forward(self, query, key, value, mask=None):\n","    \"Implements Figure 2\"\n","    if mask is not None:\n","      # Same mask applied to all h heads.\n","      mask = mask.unsqueeze(1)\n","    nbatches = query.size(0)\n","\n","    # 1) Do all the linear projections in batch from d_model => h x d_k \n","    query, key, value = \\\n","      [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n","       for l, x in zip(self.linears, (query, key, value))]\n","\n","    # 2) Apply attention on all the projected vectors in batch. \n","    x, self.attn = attention(query, key, value, mask=mask, \n","                             dropout=self.dropout)\n","\n","    # 3) \"Concat\" using a view and apply a final linear. \n","    x = x.transpose(1, 2).contiguous() \\\n","         .view(nbatches, -1, self.h * self.d_k)\n","    return self.linears[-1](x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qUPdzR7WR8T1","colab_type":"code","colab":{}},"source":["class PositionwiseFeedForward(nn.Module):\n","  \"Implements FFN equation.\"\n","  def __init__(self, d_model, d_ff, dropout=0.1):\n","    super(PositionwiseFeedForward, self).__init__()\n","    self.w_1 = nn.Linear(d_model, d_ff)\n","    self.w_2 = nn.Linear(d_ff, d_model)\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, x):\n","    return self.w_2(self.dropout(F.relu(self.w_1(x))))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sTeyPiZQR9tv","colab_type":"code","colab":{}},"source":["class Embeddings(nn.Module):\n","  def __init__(self, d_model, vocab):\n","    super(Embeddings, self).__init__()\n","    self.lut = nn.Embedding(vocab, d_model)\n","    self.d_model = d_model\n","\n","  def forward(self, x):\n","    return self.lut(x) * math.sqrt(self.d_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RoRCMB8CSFyw","colab_type":"code","colab":{}},"source":["class PositionalEncoding(nn.Module):\n","  \"Implement the PE function.\"\n","  def __init__(self, d_model, dropout, max_len=5000):\n","    super(PositionalEncoding, self).__init__()\n","    self.dropout = nn.Dropout(p=dropout)\n","\n","    # Compute the positional encodings once in log space.\n","    pe = torch.zeros(max_len, d_model)\n","    position = torch.arange(0, max_len).unsqueeze(1)\n","    div_term = torch.exp(torch.arange(0., d_model, 2) *\n","                         -(math.log(10000.0) / d_model))\n","    pe[:, 0::2] = torch.sin(position * div_term)\n","    pe[:, 1::2] = torch.cos(position * div_term)\n","    pe = pe.unsqueeze(0)\n","    self.register_buffer('pe', pe)\n","\n","  def forward(self, x):\n","    x = x + Variable(self.pe[:, :x.size(1)], \n","                     requires_grad=False)\n","    return self.dropout(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bYP48kXgSMDV","colab_type":"code","outputId":"cf1f8f30-438b-43d2-8e1d-a9616f69f96f","executionInfo":{"status":"error","timestamp":1571027251680,"user_tz":-480,"elapsed":931,"user":{"displayName":"lei teng","photoUrl":"","userId":"02755924084880047078"}},"colab":{"base_uri":"https://localhost:8080/","height":372}},"source":["plt.figure(figsize=(15, 5))\n","pe = PositionalEncoding(20, 0)\n","y = pe.forward(Variable(torch.zeros(1, 100, 20)))\n","plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n","plt.legend([\"dim %d\"%p for p in [4,5,6,7]])\n","None"],"execution_count":18,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-9b6d8737c180>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPositionalEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dim %d\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-df4c457f22aa>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, d_model, dropout, max_len)\u001b[0m\n\u001b[1;32m     10\u001b[0m     div_term = torch.exp(torch.arange(0., d_model, 2) *\n\u001b[1;32m     11\u001b[0m                          -(math.log(10000.0) / d_model))\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mpe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdiv_term\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mpe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdiv_term\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: expected device cpu and dtype Float but got device cpu and dtype Long"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 0 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"XqcALrWKSPDx","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}