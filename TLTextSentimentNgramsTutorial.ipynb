{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"“text_sentiment_ngrams_tutorial.ipynb”的副本","provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/text_sentiment_ngrams_tutorial.ipynb","timestamp":1567153440703}]},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"Tgo5mCxun6EZ","colab_type":"code","outputId":"6a45621f-7042-4731-b33a-1e332e72017e","executionInfo":{"status":"ok","timestamp":1567153474860,"user_tz":-480,"elapsed":6736,"user":{"displayName":"lei teng","photoUrl":"","userId":"02755924084880047078"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["!pip install torch<=1.2.0\n","!pip install torchtext\n","%matplotlib inline"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/bin/bash: =1.2.0: No such file or directory\n","Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.16.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.28.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.21.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.1.0)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2019.6.16)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kIbufErOn6Ec","colab_type":"text"},"source":["\n","Text Classification Tutorial\n","============================\n","\n","This tutorial shows how to use the text classification datasets,\n","including\n","\n","::\n","\n","   - AG_NEWS,\n","   - SogouNews, \n","   - DBpedia, \n","   - YelpReviewPolarity,\n","   - YelpReviewFull, \n","   - YahooAnswers, \n","   - AmazonReviewPolarity,\n","   - AmazonReviewFull\n","\n","This example shows the application of ``TextClassification`` Dataset for\n","supervised learning analysis.\n","\n","Load data with ngrams\n","---------------------\n","\n","A bag of ngrams feature is applied to capture some partial information\n","about the local word order. In practice, bi-gram or tri-gram are applied\n","to provide more benefits as word groups than only one word. An example:\n","\n","::\n","\n","   \"load data with ngrams\"\n","   Bi-grams results: \"load data\", \"data with\", \"with ngrams\"\n","   Tri-grams results: \"load data with\", \"data with ngrams\"\n","\n","``TextClassification`` Dataset supports the ngrams method. By setting\n","ngrams to 2, the example text in the dataset will be a list of single\n","words plus bi-grams string.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"HPiyh51Nn6Ed","colab_type":"code","outputId":"dc7140f5-1c3c-4dd3-84bf-85f683a411c9","executionInfo":{"status":"error","timestamp":1567153479023,"user_tz":-480,"elapsed":10891,"user":{"displayName":"lei teng","photoUrl":"","userId":"02755924084880047078"}},"colab":{"base_uri":"https://localhost:8080/","height":372}},"source":["import torch\n","import torchtext\n","from torchtext.datasets import text_classification\n","\n","NGRAMS = 2\n","import os\n","if not os.path.isdir('./.data'):\n","\tos.mkdir('./.data')\n","train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n","    root='./.data', ngrams=NGRAMS, vocab=None)\n","BATCH_SIZE = 16\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":0,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-f2a81590e1e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtext_classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mNGRAMS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'text_classification'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"6_xro4cUn6Ef","colab_type":"text"},"source":["Define the model\n","----------------\n","\n","The model is composed of the\n","`EmbeddingBag <https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag>`__\n","layer and the linear layer (see the figure below). ``nn.EmbeddingBag``\n","computes the mean value of a “bag” of embeddings. The text entries here\n","have different lengths. ``nn.EmbeddingBag`` requires no padding here\n","since the text lengths are saved in offsets.\n","\n","Additionally, since ``nn.EmbeddingBag`` accumulates the average across\n","the embeddings on the fly, ``nn.EmbeddingBag`` can enhance the\n","performance and memory efficiency to process a sequence of tensors.\n","\n","![](https://github.com/pytorch/tutorials/blob/gh-pages/_static/img/text_sentiment_ngrams_model.png?raw=1)\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"zTXm6dHIn6Eg","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","class TextSentiment(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, num_class):\n","        super().__init__()\n","        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n","        self.fc = nn.Linear(embed_dim, num_class)\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        initrange = 0.5\n","        self.embedding.weight.data.uniform_(-initrange, initrange)\n","        self.fc.weight.data.uniform_(-initrange, initrange)\n","        self.fc.bias.data.zero_()\n","        \n","    def forward(self, text, offsets):\n","        embedded = self.embedding(text, offsets)\n","        return self.fc(embedded)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t2jQrO1gn6Ei","colab_type":"text"},"source":["Initiate an instance\n","--------------------\n","\n","The AG_NEWS dataset has four labels and therefore the number of classes\n","is four.\n","\n","::\n","\n","   1 : World\n","   2 : Sports\n","   3 : Business\n","   4 : Sci/Tec\n","\n","The vocab size is equal to the length of vocab (including single word\n","and ngrams). The number of classes is equal to the number of labels,\n","which is four in AG_NEWS case.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"IBzxeeLbn6Ej","colab_type":"code","colab":{}},"source":["VOCAB_SIZE = len(train_dataset.get_vocab())\n","EMBED_DIM = 32\n","NUN_CLASS = len(train_dataset.get_labels())\n","model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eb5EW6LMn6Em","colab_type":"text"},"source":["Functions used to generate batch\n","--------------------------------\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zYn0kTgkn6En","colab_type":"text"},"source":["Since the text entries have different lengths, a custom function\n","generate_batch() is used to generate data batches and offsets. The\n","function is passed to ``collate_fn`` in ``torch.utils.data.DataLoader``.\n","The input to ``collate_fn`` is a list of tensors with the size of\n","batch_size, and the ``collate_fn`` function packs them into a\n","mini-batch. Pay attention here and make sure that ``collate_fn`` is\n","declared as a top level def. This ensures that the function is available\n","in each worker.\n","\n","The text entries in the original data batch input are packed into a list\n","and concatenated as a single tensor as the input of ``nn.EmbeddingBag``.\n","The offsets is a tensor of delimiters to represent the beginning index\n","of the individual sequence in the text tensor. Label is a tensor saving\n","the labels of individual text entries.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"joX4qtcJn6En","colab_type":"code","colab":{}},"source":["def generate_batch(batch):\n","    label = torch.tensor([entry[0] for entry in batch])\n","    text = [entry[1] for entry in batch]\n","    offsets = [0] + [len(entry) for entry in text]\n","    # torch.Tensor.cumsum returns the cumulative sum\n","    # of elements in the dimension dim.\n","    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n","    \n","    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n","    text = torch.cat(text)\n","    return text, offsets, label"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tK68a0Nsn6Ep","colab_type":"text"},"source":["Define functions to train the model and evaluate results.\n","---------------------------------------------------------\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JckvPkeRn6Er","colab_type":"text"},"source":["`torch.utils.data.DataLoader <https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader>`__\n","is recommended for PyTorch users, and it makes data loading in parallel\n","easily (a tutorial is\n","`here <https://pytorch.org/tutorials/beginner/data_loading_tutorial.html>`__).\n","We use ``DataLoader`` here to load AG_NEWS datasets and send it to the\n","model for training/validation.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"CUiusrUYn6Er","colab_type":"code","colab":{}},"source":["from torch.utils.data import DataLoader\n","\n","def train_func(sub_train_):\n","\n","    # Train the model\n","    train_loss = 0\n","    train_acc = 0\n","    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n","                      collate_fn=generate_batch)\n","    for i, (text, offsets, cls) in enumerate(data):\n","        optimizer.zero_grad()\n","        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n","        output = model(text, offsets)\n","        loss = criterion(output, cls)\n","        train_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","        train_acc += (output.argmax(1) == cls).sum().item()\n","\n","    # Adjust the learning rate\n","    scheduler.step()\n","    \n","    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n","\n","def test(data_):\n","    loss = 0\n","    acc = 0\n","    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n","    for text, offsets, cls in data:\n","        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n","        with torch.no_grad():\n","            output = model(text, offsets)\n","            loss = criterion(output, cls)\n","            loss += loss.item()\n","            acc += (output.argmax(1) == cls).sum().item()\n","\n","    return loss / len(data_), acc / len(data_)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"erldNY_hn6Ew","colab_type":"text"},"source":["Split the dataset and run the model\n","-----------------------------------\n","\n","Since the original AG_NEWS has no valid dataset, we split the training\n","dataset into train/valid sets with a split ratio of 0.95 (train) and\n","0.05 (valid). Here we use\n","`torch.utils.data.dataset.random_split <https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split>`__\n","function in PyTorch core library.\n","\n","`CrossEntropyLoss <https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss>`__\n","criterion combines nn.LogSoftmax() and nn.NLLLoss() in a single class.\n","It is useful when training a classification problem with C classes.\n","`SGD <https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html>`__\n","implements stochastic gradient descent method as optimizer. The initial\n","learning rate is set to 4.0.\n","`StepLR <https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR>`__\n","is used here to adjust the learning rate through epochs.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"FKDTHxtQn6Ex","colab_type":"code","colab":{}},"source":["import time\n","from torch.utils.data.dataset import random_split\n","N_EPOCHS = 5\n","min_valid_loss = float('inf')\n","\n","criterion = torch.nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n","\n","train_len = int(len(train_dataset) * 0.95)\n","sub_train_, sub_valid_ = \\\n","    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    train_loss, train_acc = train_func(sub_train_)\n","    valid_loss, valid_acc = test(sub_valid_)\n","\n","    secs = int(time.time() - start_time)\n","    mins = secs / 60\n","    secs = secs % 60\n","\n","    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n","    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n","    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Qa9-y18n6E0","colab_type":"text"},"source":["Running the model on GPU with the following information:\n","\n","Epoch: 1 \\| time in 0 minutes, 11 seconds\n","\n","::\n","\n","       Loss: 0.0263(train)     |       Acc: 84.5%(train)\n","       Loss: 0.0001(valid)     |       Acc: 89.0%(valid)\n","\n","\n","Epoch: 2 \\| time in 0 minutes, 10 seconds\n","\n","::\n","\n","       Loss: 0.0119(train)     |       Acc: 93.6%(train)\n","       Loss: 0.0000(valid)     |       Acc: 89.6%(valid)\n","\n","\n","Epoch: 3 \\| time in 0 minutes, 9 seconds\n","\n","::\n","\n","       Loss: 0.0069(train)     |       Acc: 96.4%(train)\n","       Loss: 0.0000(valid)     |       Acc: 90.5%(valid)\n","\n","\n","Epoch: 4 \\| time in 0 minutes, 11 seconds\n","\n","::\n","\n","       Loss: 0.0038(train)     |       Acc: 98.2%(train)\n","       Loss: 0.0000(valid)     |       Acc: 90.4%(valid)\n","\n","\n","Epoch: 5 \\| time in 0 minutes, 11 seconds\n","\n","::\n","\n","       Loss: 0.0022(train)     |       Acc: 99.0%(train)\n","       Loss: 0.0000(valid)     |       Acc: 91.0%(valid)        \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"x0cojZIzn6E4","colab_type":"text"},"source":["Evaluate the model with test dataset\n","------------------------------------\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"eolRYKoPn6E5","colab_type":"code","colab":{}},"source":["print('Checking the results of test dataset...')\n","test_loss, test_acc = test(test_dataset)\n","print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ABR5wj5On6E7","colab_type":"text"},"source":["Checking the results of test dataset…\n","\n","::\n","\n","       Loss: 0.0237(test)      |       Acc: 90.5%(test)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uDhxVJuAn6E8","colab_type":"text"},"source":["Test on a random news\n","---------------------\n","\n","Use the best model so far and test a golf news. The label information is\n","available\n","`here <https://pytorch.org/text/datasets.html?highlight=ag_news#torchtext.datasets.AG_NEWS>`__.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"cWbI1jtZn6E8","colab_type":"code","colab":{}},"source":["import re\n","from torchtext.data.utils import ngrams_iterator\n","from torchtext.data.utils import get_tokenizer\n","\n","ag_news_label = {1 : \"World\",\n","                 2 : \"Sports\",\n","                 3 : \"Business\",\n","                 4 : \"Sci/Tec\"}\n","\n","def predict(text, model, vocab, ngrams):\n","    tokenizer = get_tokenizer(\"basic_english\")\n","    with torch.no_grad():\n","        text = torch.tensor([vocab[token]\n","                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n","        output = model(text, torch.tensor([0]))\n","        return output.argmax(1).item() + 1\n","\n","ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n","    enduring the season’s worst weather conditions on Sunday at The \\\n","    Open on his way to a closing 75 at Royal Portrush, which \\\n","    considering the wind and the rain was a respectable showing. \\\n","    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n","    was another story. With temperatures in the mid-80s and hardly any \\\n","    wind, the Spaniard was 13 strokes better in a flawless round. \\\n","    Thanks to his best putting performance on the PGA Tour, Rahm \\\n","    finished with an 8-under 62 for a three-stroke lead, which \\\n","    was even more impressive considering he’d never played the \\\n","    front nine at TPC Southwind.\"\n","\n","vocab = train_dataset.get_vocab()\n","model = model.to(\"cpu\")\n","\n","print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, model, vocab, 2)])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GbLHU7BQn6E-","colab_type":"text"},"source":["This is a Sports news\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"czf8irPEn6E_","colab_type":"text"},"source":["You can find the code examples displayed in this note\n","`here <https://github.com/pytorch/text/tree/master/examples/text_classification>`__.\n","\n","\n"]}]}